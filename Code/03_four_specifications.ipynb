{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jamie Dimon Response Simulation - Four Specifications\n",
    "\n",
    "This notebook implements four different approaches to generate Jamie Dimon's responses:\n",
    "1. **Spec 1**: Random 500 Q&A pairs\n",
    "2. **Spec 2**: Most Recent 500 Q&A pairs\n",
    "3. **Spec 3**: Most Recent 500 Q&A + 10-Q Summary\n",
    "4. **Spec 4**: Persona Summary + 10-Q Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in CSV: 68866\n",
      "Columns: ['transcriptcreationdateutc', 'companyid', 'companyname', 'keydevid', 'headline', 'mostimportantdateutc', 'transcriptid', 'transcriptcollectiontypeid', 'transcriptpresentationtypeid', 'transcriptcomponentid', 'componentorder', 'transcriptcomponenttypeid', 'transcriptcomponenttypename', 'transcriptpersonname', 'speaker_companyname', 'speakertypeid', 'speakertypename', 'componenttext']\n"
     ]
    }
   ],
   "source": [
    "# Load training data (historical Q&A pairs before 2025)\n",
    "df = pd.read_csv(\"../Data/CIQ transcripts/46625H100.csv\", low_memory=False)\n",
    "\n",
    "print(f\"Total rows in CSV: {len(df)}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Deduplicating Transcripts ===\n",
      "Rows after deduplication: 18635\n",
      "\n",
      "Total Q&A components: 15947\n",
      "  Questions: 6004\n",
      "  Answers: 9943\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate and prepare data\n",
    "print(\"\\n=== Deduplicating Transcripts ===\")\n",
    "\n",
    "# Convert dates\n",
    "df['call_date'] = pd.to_datetime(df['mostimportantdateutc']).dt.date\n",
    "df['creation_date'] = pd.to_datetime(df['transcriptcreationdateutc'])\n",
    "\n",
    "# Keep only the most recent version of each earnings call\n",
    "df_grouped = df.groupby('call_date')\n",
    "latest_transcripts = []\n",
    "\n",
    "for call_date, group in df_grouped:\n",
    "    latest_creation = group['creation_date'].max()\n",
    "    latest_group = group[group['creation_date'] == latest_creation]\n",
    "    latest_transcripts.append(latest_group)\n",
    "\n",
    "df = pd.concat(latest_transcripts, ignore_index=True)\n",
    "print(f\"Rows after deduplication: {len(df)}\")\n",
    "\n",
    "# Prepare data for Q&A extraction\n",
    "df = df[['call_date', 'transcriptcomponenttypename', 'transcriptpersonname', 'speakertypename', 'componenttext']].copy()\n",
    "df.rename(columns={'call_date': 'date'}, inplace=True)\n",
    "df['transcriptpersonname'] = df['transcriptpersonname'].str.lower().str.strip()\n",
    "\n",
    "# Keep only Q&A section components\n",
    "qa_df = df[df['transcriptcomponenttypename'].isin(['Question', 'Answer'])].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTotal Q&A components: {len(qa_df)}\")\n",
    "print(f\"  Questions: {len(qa_df[qa_df['transcriptcomponenttypename'] == 'Question'])}\")\n",
    "print(f\"  Answers: {len(qa_df[qa_df['transcriptcomponenttypename'] == 'Answer'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Data Summary ===\n",
      "Total Q&A pairs extracted: 2115\n",
      "Unique call dates: 101\n",
      "Date range: 2007-07-18 to 2024-10-11\n"
     ]
    }
   ],
   "source": [
    "# Extract Q&A pairs\n",
    "train_qa_pairs = []\n",
    "\n",
    "grouped = qa_df.groupby('date')\n",
    "\n",
    "for date, group in grouped:\n",
    "    group = group.reset_index(drop=True)\n",
    "    \n",
    "    for i in range(len(group)):\n",
    "        row = group.loc[i]\n",
    "        \n",
    "        # If this is Jamie Dimon's answer\n",
    "        if row['transcriptcomponenttypename'] == 'Answer' and 'james dimon' in str(row['transcriptpersonname']).lower():\n",
    "            # Look for the question immediately before it\n",
    "            if i > 0:\n",
    "                prev_row = group.loc[i - 1]\n",
    "                \n",
    "                # Check if previous row is a question from an analyst\n",
    "                if prev_row['transcriptcomponenttypename'] == 'Question' and prev_row['speakertypename'] == 'Analysts':\n",
    "                    train_qa_pairs.append({\n",
    "                        'date': str(date),\n",
    "                        'question': prev_row['componenttext'].strip(),\n",
    "                        'answer': row['componenttext'].strip()\n",
    "                    })\n",
    "\n",
    "print(f\"\\n=== Training Data Summary ===\")\n",
    "print(f\"Total Q&A pairs extracted: {len(train_qa_pairs)}\")\n",
    "print(f\"Unique call dates: {len(set([pair['date'] for pair in train_qa_pairs]))}\")\n",
    "if train_qa_pairs:\n",
    "    print(f\"Date range: {min([pair['date'] for pair in train_qa_pairs])} to {max([pair['date'] for pair in train_qa_pairs])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-2025 Q&A pairs: 2115\n",
      "Date range: 2007-07-18 to 2024-10-11\n"
     ]
    }
   ],
   "source": [
    "# Filter to only pre-2025 data for training\n",
    "train_qa_pairs_pre_2025 = [pair for pair in train_qa_pairs if pair['date'] < '2025-01-01']\n",
    "print(f\"\\nPre-2025 Q&A pairs: {len(train_qa_pairs_pre_2025)}\")\n",
    "print(f\"Date range: {min([pair['date'] for pair in train_qa_pairs_pre_2025])} to {max([pair['date'] for pair in train_qa_pairs_pre_2025])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Data Summary ===\n",
      "Testing Q&A pairs loaded: 43\n",
      "Test data quarters: {'2025Q1', '2025Q2', '2025Q3'}\n"
     ]
    }
   ],
   "source": [
    "# Load test data (2025 Q1-Q3)\n",
    "test_qa_pairs = []\n",
    "with open(\"../Processed Data/test_qa_pairs.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        test_qa_pairs.append(json.loads(line))\n",
    "\n",
    "print(f\"\\n=== Testing Data Summary ===\")\n",
    "print(f\"Testing Q&A pairs loaded: {len(test_qa_pairs)}\")\n",
    "print(f\"Test data quarters: {set([pair['quarter'] for pair in test_qa_pairs])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 10-Q Summaries Loaded ===\n",
      "Available quarters: ['2025Q1', '2025Q2', '2025Q3']\n"
     ]
    }
   ],
   "source": [
    "# Load 10-Q summaries\n",
    "with open(\"../Processed Data/10Q_summaries.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summaries_data = json.load(f)\n",
    "\n",
    "# Create a dictionary for easy lookup by quarter\n",
    "summaries_dict = {}\n",
    "for summary in summaries_data['summaries']:\n",
    "    summaries_dict[summary['quarter']] = summary['summary']\n",
    "\n",
    "print(f\"\\n=== 10-Q Summaries Loaded ===\")\n",
    "print(f\"Available quarters: {list(summaries_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    \"\"\"Count the number of tokens in a text string\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def save_results(results, spec_name, output_dir=\"../Results\"):\n",
    "    \"\"\"Save results to JSON file\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{spec_name}_{timestamp}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Results saved to: {filepath}\")\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Specification 1: Random 500 Q&A Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_spec1(past_qa_pairs, new_question, max_tokens=120000):\n",
    "    \"\"\"\n",
    "    Build prompt for Specification 1: Random 500 Q&A pairs\n",
    "    \"\"\"\n",
    "    prompt_header = \"You are Jamie Dimon. Based on your past earnings call responses, answer the following analyst question in your usual tone and style.\\n\\n\"\n",
    "    prompt_header += \"PAST RESPONSES:\\n\"\n",
    "    \n",
    "    prompt_footer = f\"\\nCURRENT QUESTION:\\nQ: {new_question}\\n\\nYour Answer:\"\n",
    "    \n",
    "    # Count tokens for fixed parts\n",
    "    header_tokens = count_tokens(prompt_header)\n",
    "    footer_tokens = count_tokens(prompt_footer)\n",
    "    fixed_tokens = header_tokens + footer_tokens\n",
    "    \n",
    "    # Build examples section\n",
    "    examples_text = \"\"\n",
    "    examples_used = 0\n",
    "    total_tokens = fixed_tokens\n",
    "    \n",
    "    for pair in past_qa_pairs:\n",
    "        example_text = f\"Q: {pair['question']}\\nA: {pair['answer']}\\n\\n\"\n",
    "        example_tokens = count_tokens(example_text)\n",
    "        \n",
    "        if total_tokens + example_tokens > max_tokens:\n",
    "            break\n",
    "        \n",
    "        examples_text += example_text\n",
    "        total_tokens += example_tokens\n",
    "        examples_used += 1\n",
    "    \n",
    "    prompt = prompt_header + examples_text + prompt_footer\n",
    "    \n",
    "    return prompt, total_tokens, examples_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SPECIFICATION 1: Random 500 Q&A Pairs\n",
      "================================================================================\n",
      "\n",
      "Sampled 500 random Q&A pairs\n",
      "Date range: 2007-07-18 to 2024-10-11\n",
      "\n",
      "Generating responses for 43 test questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spec 1: 100%|██████████| 43/43 [05:55<00:00,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: ../Results/spec1_random_500_20251110_180802.json\n",
      "\n",
      "Spec 1 completed! 43 responses generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Specification 1: Random 500 Q&A\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPECIFICATION 1: Random 500 Q&A Pairs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Randomly sample 500 Q&A pairs from pre-2025 data\n",
    "random.seed(42)  # For reproducibility\n",
    "random_500_qa = random.sample(train_qa_pairs_pre_2025, min(500, len(train_qa_pairs_pre_2025)))\n",
    "\n",
    "print(f\"\\nSampled {len(random_500_qa)} random Q&A pairs\")\n",
    "print(f\"Date range: {min([pair['date'] for pair in random_500_qa])} to {max([pair['date'] for pair in random_500_qa])}\")\n",
    "\n",
    "# Generate responses for all test questions\n",
    "spec1_results = {\n",
    "    \"specification\": \"spec1_random_500\",\n",
    "    \"description\": \"Random 500 Q&A pairs from historical data\",\n",
    "    \"training_samples\": len(random_500_qa),\n",
    "    \"test_samples\": len(test_qa_pairs),\n",
    "    \"generated_date\": datetime.now().isoformat(),\n",
    "    \"results\": []\n",
    "}\n",
    "\n",
    "print(f\"\\nGenerating responses for {len(test_qa_pairs)} test questions...\")\n",
    "\n",
    "for idx, test_pair in enumerate(tqdm(test_qa_pairs, desc=\"Spec 1\")):\n",
    "    try:\n",
    "        # Build prompt\n",
    "        prompt, token_count, examples_used = build_prompt_spec1(\n",
    "            random_500_qa,\n",
    "            test_pair['question']\n",
    "        )\n",
    "        \n",
    "        # Generate response\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        generated_answer = response.choices[0].message.content\n",
    "        \n",
    "        # Store result\n",
    "        spec1_results['results'].append({\n",
    "            \"test_id\": idx,\n",
    "            \"quarter\": test_pair['quarter'],\n",
    "            \"date\": test_pair['date'],\n",
    "            \"analyst\": test_pair['analyst'],\n",
    "            \"question\": test_pair['question'],\n",
    "            \"ground_truth_answer\": test_pair['answer'],\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"prompt_tokens\": token_count,\n",
    "            \"examples_used\": examples_used\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing test question {idx}: {str(e)}\")\n",
    "        spec1_results['results'].append({\n",
    "            \"test_id\": idx,\n",
    "            \"quarter\": test_pair['quarter'],\n",
    "            \"date\": test_pair['date'],\n",
    "            \"analyst\": test_pair['analyst'],\n",
    "            \"question\": test_pair['question'],\n",
    "            \"ground_truth_answer\": test_pair['answer'],\n",
    "            \"generated_answer\": f\"ERROR: {str(e)}\",\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# Save results\n",
    "spec1_filepath = save_results(spec1_results, \"spec1_random_500\")\n",
    "print(f\"\\nSpec 1 completed! {len(spec1_results['results'])} responses generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Specification 2: Most Recent 500 Q&A Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SPECIFICATION 2: Most Recent 500 Q&A Pairs\n",
      "================================================================================\n",
      "\n",
      "Selected 500 most recent Q&A pairs\n",
      "Date range: 2019-09-10 to 2024-10-11\n",
      "\n",
      "Generating responses for 43 test questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spec 2: 100%|██████████| 43/43 [05:27<00:00,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: ../Results/spec2_recent_500_20251110_181330.json\n",
      "\n",
      "Spec 2 completed! 43 responses generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Specification 2: Most Recent 500 Q&A\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPECIFICATION 2: Most Recent 500 Q&A Pairs\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sort by date and take the most recent 500\n",
    "sorted_qa = sorted(train_qa_pairs_pre_2025, key=lambda x: x['date'], reverse=True)\n",
    "recent_500_qa = sorted_qa[:500]\n",
    "\n",
    "print(f\"\\nSelected {len(recent_500_qa)} most recent Q&A pairs\")\n",
    "print(f\"Date range: {min([pair['date'] for pair in recent_500_qa])} to {max([pair['date'] for pair in recent_500_qa])}\")\n",
    "\n",
    "# Generate responses\n",
    "spec2_results = {\n",
    "    \"specification\": \"spec2_recent_500\",\n",
    "    \"description\": \"Most recent 500 Q&A pairs prior to 2025\",\n",
    "    \"training_samples\": len(recent_500_qa),\n",
    "    \"test_samples\": len(test_qa_pairs),\n",
    "    \"generated_date\": datetime.now().isoformat(),\n",
    "    \"results\": []\n",
    "}\n",
    "\n",
    "print(f\"\\nGenerating responses for {len(test_qa_pairs)} test questions...\")\n",
    "\n",
    "for idx, test_pair in enumerate(tqdm(test_qa_pairs, desc=\"Spec 2\")):\n",
    "    try:\n",
    "        # Build prompt (same as Spec 1, but with recent_500_qa)\n",
    "        prompt, token_count, examples_used = build_prompt_spec1(\n",
    "            recent_500_qa,\n",
    "            test_pair['question']\n",
    "        )\n",
    "        \n",
    "        # Generate response\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        generated_answer = response.choices[0].message.content\n",
    "        \n",
    "        # Store result\n",
    "        spec2_results['results'].append({\n",
    "            \"test_id\": idx,\n",
    "            \"quarter\": test_pair['quarter'],\n",
    "            \"date\": test_pair['date'],\n",
    "            \"analyst\": test_pair['analyst'],\n",
    "            \"question\": test_pair['question'],\n",
    "            \"ground_truth_answer\": test_pair['answer'],\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"prompt_tokens\": token_count,\n",
    "            \"examples_used\": examples_used\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing test question {idx}: {str(e)}\")\n",
    "        spec2_results['results'].append({\n",
    "            \"test_id\": idx,\n",
    "            \"quarter\": test_pair['quarter'],\n",
    "            \"date\": test_pair['date'],\n",
    "            \"analyst\": test_pair['analyst'],\n",
    "            \"question\": test_pair['question'],\n",
    "            \"ground_truth_answer\": test_pair['answer'],\n",
    "            \"generated_answer\": f\"ERROR: {str(e)}\",\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# Save results\n",
    "spec2_filepath = save_results(spec2_results, \"spec2_recent_500\")\n",
    "print(f\"\\nSpec 2 completed! {len(spec2_results['results'])} responses generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Specification 3: Most Recent 500 Q&A + 10-Q Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_spec3(past_qa_pairs, new_question, quarter_summary, max_tokens=120000):\n",
    "    \"\"\"\n",
    "    Build prompt for Specification 3: Most Recent 500 Q&A + 10-Q Summary\n",
    "    \"\"\"\n",
    "    prompt_header = \"You are Jamie Dimon. Based on your past earnings call responses and the current quarter's financial report, answer the following analyst question in your usual tone and style.\\n\\n\"\n",
    "    \n",
    "    # Add 10-Q summary\n",
    "    prompt_header += f\"CURRENT QUARTER 10-Q SUMMARY:\\n{quarter_summary}\\n\\n\"\n",
    "    prompt_header += \"PAST RESPONSES:\\n\"\n",
    "    \n",
    "    prompt_footer = f\"\\nCURRENT QUESTION:\\nQ: {new_question}\\n\\nYour Answer:\"\n",
    "    \n",
    "    # Count tokens for fixed parts\n",
    "    header_tokens = count_tokens(prompt_header)\n",
    "    footer_tokens = count_tokens(prompt_footer)\n",
    "    fixed_tokens = header_tokens + footer_tokens\n",
    "    \n",
    "    # Build examples section\n",
    "    examples_text = \"\"\n",
    "    examples_used = 0\n",
    "    total_tokens = fixed_tokens\n",
    "    \n",
    "    for pair in past_qa_pairs:\n",
    "        example_text = f\"Q: {pair['question']}\\nA: {pair['answer']}\\n\\n\"\n",
    "        example_tokens = count_tokens(example_text)\n",
    "        \n",
    "        if total_tokens + example_tokens > max_tokens:\n",
    "            break\n",
    "        \n",
    "        examples_text += example_text\n",
    "        total_tokens += example_tokens\n",
    "        examples_used += 1\n",
    "    \n",
    "    prompt = prompt_header + examples_text + prompt_footer\n",
    "    \n",
    "    return prompt, total_tokens, examples_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SPECIFICATION 3: Most Recent 500 Q&A + 10-Q Summary\n",
      "================================================================================\n",
      "\n",
      "Generating responses for 43 test questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spec 3: 100%|██████████| 43/43 [06:40<00:00,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: ../Results/spec3_recent_500_plus_10q_20251110_182010.json\n",
      "\n",
      "Spec 3 completed! 43 responses generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Specification 3: Most Recent 500 Q&A + 10-Q Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPECIFICATION 3: Most Recent 500 Q&A + 10-Q Summary\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "spec3_results = {\n",
    "    \"specification\": \"spec3_recent_500_plus_10q\",\n",
    "    \"description\": \"Most recent 500 Q&A pairs + quarterly 10-Q summary\",\n",
    "    \"training_samples\": len(recent_500_qa),\n",
    "    \"test_samples\": len(test_qa_pairs),\n",
    "    \"generated_date\": datetime.now().isoformat(),\n",
    "    \"results\": []\n",
    "}\n",
    "\n",
    "print(f\"\\nGenerating responses for {len(test_qa_pairs)} test questions...\")\n",
    "\n",
    "for idx, test_pair in enumerate(tqdm(test_qa_pairs, desc=\"Spec 3\")):\n",
    "    try:\n",
    "        # Get the appropriate 10-Q summary for this quarter\n",
    "        quarter = test_pair['quarter']\n",
    "        quarter_summary = summaries_dict.get(quarter, \"No 10-Q summary available for this quarter.\")\n",
    "        \n",
    "        # Build prompt with 10-Q summary\n",
    "        prompt, token_count, examples_used = build_prompt_spec3(\n",
    "            recent_500_qa,\n",
    "            test_pair['question'],\n",
    "            quarter_summary\n",
    "        )\n",
    "        \n",
    "        # Generate response\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        generated_answer = response.choices[0].message.content\n",
    "        \n",
    "        # Store result\n",
    "        spec3_results['results'].append({\n",
    "            \"test_id\": idx,\n",
    "            \"quarter\": test_pair['quarter'],\n",
    "            \"date\": test_pair['date'],\n",
    "            \"analyst\": test_pair['analyst'],\n",
    "            \"question\": test_pair['question'],\n",
    "            \"ground_truth_answer\": test_pair['answer'],\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"prompt_tokens\": token_count,\n",
    "            \"examples_used\": examples_used,\n",
    "            \"10q_summary_included\": True\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing test question {idx}: {str(e)}\")\n",
    "        spec3_results['results'].append({\n",
    "            \"test_id\": idx,\n",
    "            \"quarter\": test_pair['quarter'],\n",
    "            \"date\": test_pair['date'],\n",
    "            \"analyst\": test_pair['analyst'],\n",
    "            \"question\": test_pair['question'],\n",
    "            \"ground_truth_answer\": test_pair['answer'],\n",
    "            \"generated_answer\": f\"ERROR: {str(e)}\",\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# Save results\n",
    "spec3_filepath = save_results(spec3_results, \"spec3_recent_500_plus_10q\")\n",
    "print(f\"\\nSpec 3 completed! {len(spec3_results['results'])} responses generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Specification 4: Persona Summary + 10-Q Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SPECIFICATION 4: Persona Summary + 10-Q Summary\n",
      "================================================================================\n",
      "\n",
      "Step 1: Generating Jamie Dimon's persona summary...\n",
      "Generating persona from 99,969 tokens...\n",
      "\n",
      "================================================================================\n",
      "JAMIE DIMON PERSONA SUMMARY:\n",
      "================================================================================\n",
      "### Persona Summary: Jamie Dimon\n",
      "\n",
      "**1. Communication Style and Tone:**\n",
      "Jamie Dimon’s communication style is direct, candid, and often informal. He uses plain language and is unafraid to express his opinions strongly, even when discussing complex financial topics. His tone is confident and authoritative, reflecting his deep knowledge and experience in the banking sector. Dimon is also personable and occasionally injects humor into his responses, making his communication approachable despite the technical nature of the discussions.\n",
      "\n",
      "**2. Common Themes and Topics He Emphasizes:**\n",
      "Dimon frequently emphasizes the importance of serving clients, maintaining a strong balance sheet, and being prepared for economic uncertainties. He often discusses the competitive landscape, highlighting the challenges from fintech and big tech, while also focusing on JPMorgan's strategic investments and growth opportunities. Risk management and the importance of being prepared for various economic scenarios are recurring themes, as is the need for innovation and adaptation in a rapidly changing financial environment.\n",
      "\n",
      "**3. How He Structures His Responses:**\n",
      "Dimon typically structures his responses by first acknowledging the current state or data point, then providing context or historical comparison, and finally, outlining JPMorgan’s strategic approach or future plans. He often includes anecdotes or examples to illustrate his points, making complex topics more relatable. He is methodical yet flexible, willing to pivot as new information arises.\n",
      "\n",
      "**4. His Values and Perspectives:**\n",
      "Dimon values long-term growth, client service, and maintaining a competitive edge. He is a proponent of capitalism and competition, believing in the necessity of adapting to market changes. He values transparency and accountability, evident in his acknowledgment of past mistakes and his focus on continuous improvement. Dimon also prioritizes risk management and financial stability, consistently emphasizing the importance of being prepared for economic downturns.\n",
      "\n",
      "**5. Characteristic Phrases or Expressions:**\n",
      "- \"Our capital cup runneth over\" – indicating strong capital reserves.\n",
      "- \"Gravel in the gut and spit in the eye\" – referring to the toughness needed to compete.\n",
      "- \"We're not going to sit on our hands\" – expressing the proactive nature of JPMorgan.\n",
      "- \"Silicon Valley is coming\" – highlighting the competitive threat from fintech.\n",
      "- \"We're going to do whatever it takes\" – showing commitment to maintaining leadership.\n",
      "- \"Prepared for all eventualities\" – emphasizing readiness for various economic conditions.\n",
      "\n",
      "Overall, Jamie Dimon presents as a seasoned leader who values strategic growth, innovation, and a strong financial foundation. His communication is characterized by clarity, a focus on long-term objectives, and a readiness to tackle challenges head-on.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate persona summary from most recent 500 Q&A pairs\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPECIFICATION 4: Persona Summary + 10-Q Summary\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nStep 1: Generating Jamie Dimon's persona summary...\")\n",
    "\n",
    "# Build prompt for persona generation\n",
    "persona_prompt = \"\"\"Based on the following Q&A exchanges from Jamie Dimon's earnings call responses, \n",
    "create a concise persona summary that captures his communication style, tone, key themes, \n",
    "and typical response patterns. Your analysis may include, but not be limited to, the following dimensions:\n",
    "1. Communication style and tone\n",
    "2. Common themes and topics he emphasizes\n",
    "3. How he structures his responses\n",
    "4. His values and perspectives\n",
    "5. Characteristic phrases or expressions\n",
    "\n",
    "Q&A EXAMPLES:\\n\"\"\"\n",
    "\n",
    "# Add the 500 most recent Q&A pairs (with token limit)\n",
    "max_tokens_for_persona = 100000\n",
    "current_tokens = count_tokens(persona_prompt)\n",
    "\n",
    "for pair in recent_500_qa:\n",
    "    example_text = f\"Q: {pair['question']}\\nA: {pair['answer']}\\n\\n\"\n",
    "    example_tokens = count_tokens(example_text)\n",
    "    \n",
    "    if current_tokens + example_tokens > max_tokens_for_persona:\n",
    "        break\n",
    "    \n",
    "    persona_prompt += example_text\n",
    "    current_tokens += example_tokens\n",
    "\n",
    "persona_prompt += \"\\nBased on these examples, provide a comprehensive persona summary:\"\n",
    "\n",
    "# Generate persona summary\n",
    "print(f\"Generating persona from {current_tokens:,} tokens...\")\n",
    "\n",
    "persona_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": persona_prompt}],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "jamie_persona = persona_response.choices[0].message.content\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"JAMIE DIMON PERSONA SUMMARY:\")\n",
    "print(\"=\"*80)\n",
    "print(jamie_persona)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_spec4(persona_summary, new_question, quarter_summary, max_tokens=120000):\n",
    "    \"\"\"\n",
    "    Build prompt for Specification 4: Persona Summary + 10-Q Summary\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are Jamie Dimon, CEO of JPMorgan Chase. \n",
    "\n",
    "YOUR COMMUNICATION STYLE AND PERSONA:\n",
    "{persona_summary}\n",
    "\n",
    "CURRENT QUARTER 10-Q SUMMARY:\n",
    "{quarter_summary}\n",
    "\n",
    "CURRENT QUESTION:\n",
    "Q: {new_question}\n",
    "\n",
    "Your Answer:\"\"\"\n",
    "    \n",
    "    token_count = count_tokens(prompt)\n",
    "    \n",
    "    return prompt, token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Generating responses using persona summary + 10-Q summaries...\n",
      "\n",
      "Generating responses for 43 test questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spec 4: 100%|██████████| 43/43 [03:34<00:00,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: ../Results/spec4_persona_plus_10q_20251110_182428.json\n",
      "\n",
      "Spec 4 completed! 43 responses generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Specification 4: Generate responses using persona + 10-Q\n",
    "print(\"\\nStep 2: Generating responses using persona summary + 10-Q summaries...\")\n",
    "\n",
    "spec4_results = {\n",
    "    \"specification\": \"spec4_persona_plus_10q\",\n",
    "    \"description\": \"Persona summary from 500 most recent Q&A + quarterly 10-Q summary\",\n",
    "    \"persona_summary\": jamie_persona,\n",
    "    \"test_samples\": len(test_qa_pairs),\n",
    "    \"generated_date\": datetime.now().isoformat(),\n",
    "    \"results\": []\n",
    "}\n",
    "\n",
    "print(f\"\\nGenerating responses for {len(test_qa_pairs)} test questions...\")\n",
    "\n",
    "for idx, test_pair in enumerate(tqdm(test_qa_pairs, desc=\"Spec 4\")):\n",
    "    try:\n",
    "        # Get the appropriate 10-Q summary for this quarter\n",
    "        quarter = test_pair['quarter']\n",
    "        quarter_summary = summaries_dict.get(quarter, \"No 10-Q summary available for this quarter.\")\n",
    "        \n",
    "        # Build prompt with persona + 10-Q summary\n",
    "        prompt, token_count = build_prompt_spec4(\n",
    "            jamie_persona,\n",
    "            test_pair['question'],\n",
    "            quarter_summary\n",
    "        )\n",
    "        \n",
    "        # Generate response\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        generated_answer = response.choices[0].message.content\n",
    "        \n",
    "        # Store result\n",
    "        spec4_results['results'].append({\n",
    "            \"test_id\": idx,\n",
    "            \"quarter\": test_pair['quarter'],\n",
    "            \"date\": test_pair['date'],\n",
    "            \"analyst\": test_pair['analyst'],\n",
    "            \"question\": test_pair['question'],\n",
    "            \"ground_truth_answer\": test_pair['answer'],\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"prompt_tokens\": token_count,\n",
    "            \"persona_summary_included\": True,\n",
    "            \"10q_summary_included\": True\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing test question {idx}: {str(e)}\")\n",
    "        spec4_results['results'].append({\n",
    "            \"test_id\": idx,\n",
    "            \"quarter\": test_pair['quarter'],\n",
    "            \"date\": test_pair['date'],\n",
    "            \"analyst\": test_pair['analyst'],\n",
    "            \"question\": test_pair['question'],\n",
    "            \"ground_truth_answer\": test_pair['answer'],\n",
    "            \"generated_answer\": f\"ERROR: {str(e)}\",\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# Save results\n",
    "spec4_filepath = save_results(spec4_results, \"spec4_persona_plus_10q\")\n",
    "print(f\"\\nSpec 4 completed! {len(spec4_results['results'])} responses generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
